# Adaptive Preference Scaling for RLHF

This repository contains the code for our paper [Adaptive Preference Scaling for Reinforcement Learning with Human Feedback](https://arxiv.org/abs/2406.02764). We propose an adaptive preference loss for reward modeling in the context of RLHF, which improves the flexibility of the reward learning process by adding an adaptive scaling parameter to the loss for each sample in the preference dataset.
